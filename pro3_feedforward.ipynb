{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Loading data ##\n",
    "\n",
    "# Load all training inputs to a python list\n",
    "train_inputs = []\n",
    "with open('E:\\\\MinMcGill\\\\ml\\\\project3\\\\data_and_scripts\\\\data_and_scripts\\\\train_inputs.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader, None)  # skip the header\n",
    "    for train_input in reader: \n",
    "        train_input_no_id = []\n",
    "        for pixel in train_input[1:]: # Start at index 1 to skip the Id\n",
    "            train_input_no_id.append(float(pixel))\n",
    "        train_inputs.append(train_input_no_id) \n",
    "\n",
    "# Load all training ouputs to a python list\n",
    "train_outputs = []\n",
    "with open('E:\\\\MinMcGill\\\\ml\\\\project3\\\\data_and_scripts\\\\data_and_scripts\\\\train_outputs.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader, None)  # skip the header\n",
    "    for train_output in reader:  \n",
    "        train_output_no_id = int(train_output[1])\n",
    "        train_outputs.append(train_output_no_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## convert_to_numpy ##\n",
    "# Convert python lists to numpy arrays\n",
    "train_inputs_np = np.asarray(train_inputs)\n",
    "train_outputs_np = np.asarray(train_outputs)\n",
    "\n",
    "# Save as numpy array files\n",
    "np.save('train_inputs', train_inputs_np)\n",
    "np.save('train_outputs', train_outputs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load numpy array files ##\n",
    "train_inputs = np.load(path+'\\\\train_inputs.npy')\n",
    "train_outputs = np.load(path+'\\\\train_outputs.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,50000) (2304,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b5614ce7d8f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PCA_821.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_inputs_PCA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_inputs_PCA'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mG:\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\pca.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m         \u001b[0mX_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfast_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhiten\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,50000) (2304,) "
     ]
    }
   ],
   "source": [
    "\n",
    "pca = PCA(n_components = 821)\n",
    "result = pca.fit(train_inputs)\n",
    "VR = result.explained_variance_ratio_\n",
    "\n",
    "joblib.dump(result, \"PCA_821.pkl\")\n",
    "np.save('train_inputs_PCA', result.transform(train_inputs))\n",
    "np.save('test_inputs_PCA', result.transform(train_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_inputs = np.load(path+'\\\\train_inputs_PCA.npy')\n",
    "train_outputs = np.load(path+'\\\\train_outputs_PCA.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define sigmoid function and its derivitive ##\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(x):\n",
    "    return 1.0 - np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_input, num_hidden, num_output, sigmoid_func = 'sigmoid', iterations = 10000, learning_rate = .2):\n",
    "        \"\"\"\n",
    "        :param num_input: The number of units in the input layer\n",
    "        :param num_hidden: The number of units in the hidden layer\n",
    "        :param num_output: The number of units in the output layer\n",
    "        :param sigmoid_func: The activation function to be used. Can be \"sigmoid\" or \"tanh\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # set the activation function to be used\n",
    "        if sigmoid_func == 'sigmoid':\n",
    "            self.sigmoid_func =  sigmoid\n",
    "            self.sigmoid_func_deriv = sigmoid_deriv\n",
    "        elif sigmoid_func == 'tanh':\n",
    "            self.sigmoid_func = tanh\n",
    "            self.sigmoid_func_deriv = tanh_deriv\n",
    "            \n",
    "        self.num_input = num_input\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_output = num_output\n",
    "        self.iterations = iterations\n",
    "        self.learning_rate =learning_rate\n",
    "        \n",
    "    ##set the weights of neurons in layers randomly from 0 to w, adding a bias unit at every layer\n",
    "    def set_weight(self,w):\n",
    "        self.weights = []\n",
    "        # set the weights from the input layer to the hidden layer:[-w,w), add the weight of the bias term\n",
    "        self.weights.append((2*np.random.random_sample((self.num_input + 1, self.num_hidden + 1))-1)*w)\n",
    "        # set the weights from the hidden layer to the output layer:[-w,w), add the weight of the bias term\n",
    "        self.weights.append((2*np.random.random((self.num_hidden + 1, self.num_output))-1)*w)\n",
    "                \n",
    "        \n",
    "    def fitNN(self, X, Y):\n",
    "        \n",
    "        # adding the bias term to input data\n",
    "        bias = np.ones(X.shape[0])\n",
    "        X = np.concatenate((X, bias[:,np.newaxis]), axis=1)\n",
    "        Y = np.array(Y)\n",
    "\n",
    "        ## stochastic gradient descent\n",
    "        for k in range(self.iterations):\n",
    "            \n",
    "            # pick a training example randomly\n",
    "            index_ex = np.random.randint(X.shape[0],size=50)\n",
    "            out = [X[index_ex]]\n",
    "\n",
    "            ## forward pass            \n",
    "            #compute the output of hidden units\n",
    "            out.append(self.sigmoid_func(np.dot(out[0],  self.weights[0])))\n",
    "            #compute the output of output units\n",
    "            out.append(self.sigmoid_func(np.dot(out[1],  self.weights[1])))\n",
    "            \n",
    "            ## back propogation\n",
    "            # for the output unit, compute the correction\n",
    "            deltas = [(Y[index_ex] - out[-1]) * self.sigmoid_func_deriv(out[-1])]\n",
    "\n",
    "            # for each hidden unit, compute its share of the correction\n",
    "            deltas.append(deltas[-1].dot(self.weights[1].T)*self.sigmoid_func_deriv(out[1]))\n",
    "            deltas.reverse()\n",
    "            \n",
    "            # update weights between the hidden layer and the output layer\n",
    "            for i in range(len( self.weights)):\n",
    "                 self.weights[i] += self.learning_rate * np.dot(np.matrix(out[i]).T,np.matrix(deltas[i]))/50\n",
    "                \n",
    "    def predict(self, x):\n",
    "        # adding the bias term\n",
    "        x = np.array(x)\n",
    "        x = np.concatenate((x, [1]), axis=0)\n",
    "        predictions = x\n",
    "        # forward pass to get the classification of the input instance\n",
    "        for i in range(0, len(self.weights)):\n",
    "            predictions = self.sigmoid_func(np.dot(predictions, self.weights[i]))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function of tuning parameter sets using kfold ##\n",
    "def tune_para(X,y,param,k):                      \n",
    "    KF = KFold(X_train.shape[0], n_folds=k)\n",
    "    ave_precision = []\n",
    "    for i in range (len(param)): \n",
    "        precision = []\n",
    "    \n",
    "        for train, test in KF:     \n",
    "            train_x_kf, test_x_kf, train_y_kf, test_y_kf = X[train], X[test], y[train], y[test]\n",
    "            labels_train_kf = LabelBinarizer().fit_transform(train_y_kf)\n",
    "\n",
    "            nn = NeuralNetwork(param[i][0],param[i][1],param[i][2],param[i][3],param[i][4],param[i][5])    \n",
    "            nn.set_weight(0.2)\n",
    "            nn.fitNN(train_x_kf,labels_train_kf)\n",
    "            \n",
    "            predictions = []\n",
    "            for j in range(test_x_kf.shape[0]):\n",
    "                o = nn.predict(test_x_kf[j] )\n",
    "                predictions.append(np.argmax(o))\n",
    "            precision.append( np.mean(predictions == np.ravel(test_y_kf)))\n",
    "        \n",
    "        ave_precision.append( np.sum(precision) * 100 / k)\n",
    "        print ('parameter set:', param[i], 'average precision: ',ave_precision[i], '%')\n",
    "    \n",
    "    best_param = param[np.argmax(ave_precision)]\n",
    "    print ('The best parameter set until now:', best_param)\n",
    "    return best_param\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tune the number of hidden units\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 10000, 0.2] average precision:  29.25 %\n",
      "parameter set: [2304, 5000, 10, 'sigmoid', 10000, 0.2] average precision:  29.16 %\n",
      "parameter set: [2304, 7000, 10, 'sigmoid', 10000, 0.2] average precision:  28.88 %\n",
      "The best parameter set until now: [2304, 3000, 10, 'sigmoid', 10000, 0.2]\n",
      "\n",
      "\n",
      "Tune the learning rate of gradient descent\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 10000, 0.15] average precision:  29.29 %\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 10000, 0.2] average precision:  28.87 %\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 10000, 0.25] average precision:  28.3 %\n",
      "The best parameter set until now: [2304, 3000, 10, 'sigmoid', 10000, 0.15]\n",
      "\n",
      "\n",
      "Tune the number of iterations of gradient descent\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 15000, 0.15] average precision:  28.7 %\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 20000, 0.15] average precision:  27.79 %\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 25000, 0.15] average precision:  27.65 %\n",
      "The best parameter set until now: [2304, 3000, 10, 'sigmoid', 15000, 0.15]\n",
      "\n",
      "\n",
      "Tune activatino function\n",
      "parameter set: [2304, 3000, 10, 'sigmoid', 15000, 0.15] average precision:  28.68 %\n",
      "parameter set: [2304, 3000, 10, 'tanh', 15000, 0.15] average precision:  14.39 %\n",
      "The best parameter set until now: [2304, 3000, 10, 'sigmoid', 15000, 0.15]\n",
      "\n",
      "\n",
      "The best parameters are:\n",
      "number of hidden units: 3000\n",
      "learning rate: 0.15\n",
      "number of iterations: 15000\n",
      "activatino function: sigmoid\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "X = np.array(train_inputs)\n",
    "y = np.array(train_outputs)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, train_size = 0.2, random_state=0)\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "\n",
    "#tune number of hidden units\n",
    "print('Tune the number of hidden units')\n",
    "param_set = [[2304,3000,10,'sigmoid',10000,.2],[2304,5000,10,'sigmoid',10000,.2],[2304,7000,10,'sigmoid',10000,.2]]\n",
    "best_param_set = tune_para(X_train,y_train,param_set,5)\n",
    "H = best_param_set[1]\n",
    "\n",
    "#tune learning rate\n",
    "print ('\\n')\n",
    "print ('Tune the learning rate of gradient descent')\n",
    "param_set = [[2304,H,10,'sigmoid',10000,.15],[2304,H,10,'sigmoid',10000,.2],[2304,H,10,'sigmoid',10000,.25]]\n",
    "best_param_set = tune_para(X_train,y_train,param_set,5)\n",
    "L = best_param_set[5]\n",
    "\n",
    "#tune number of iterations\n",
    "print ('\\n')\n",
    "print ('Tune the number of iterations of gradient descent')\n",
    "param_set = [[2304,H,10,'sigmoid',15000,L],[2304,H,10,'sigmoid',20000,L],[2304,H,10,'sigmoid',25000,L]]\n",
    "best_param_set = tune_para(X_train,y_train,param_set,5)\n",
    "I = best_param_set[4]\n",
    "\n",
    "#tune activation function\n",
    "print ('\\n')\n",
    "print ('Tune activatino function')\n",
    "param_set = [[2304,H,10,'sigmoid',I,L],[2304,H,10,'tanh',I,L]]\n",
    "best_param_set = tune_para(X_train,y_train,param_set,5)\n",
    "\n",
    "print ('\\n')\n",
    "print ('The best parameters are:')\n",
    "print('number of hidden units:', best_param_set[1])\n",
    "print('learning rate:', best_param_set[5])\n",
    "print('number of iterations:',best_param_set[4])\n",
    "print('activatino function:',best_param_set[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165   0  13  22   7   6  25  13   1  12]\n",
      " [  2 123  26  27  18  10  22  21  37   9]\n",
      " [ 36  26  37  27  29   7  22  26  15  14]\n",
      " [ 20  20  25  95   7  13  22   5  22   9]\n",
      " [ 12  14  17  14  71   7  30  39  14  32]\n",
      " [ 20   6  16  43  21  32  30  13  11  29]\n",
      " [ 28  10  11  10  18   9  67  32   8  20]\n",
      " [ 22  12  22  12  34  10  33  76   5  39]\n",
      " [  9  48  16  62  16  14  23   9  56  11]\n",
      " [ 18   7  13  17  32  11  35  44  18  56]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.62      0.55       264\n",
      "          1       0.46      0.42      0.44       295\n",
      "          2       0.19      0.15      0.17       239\n",
      "          3       0.29      0.40      0.34       238\n",
      "          4       0.28      0.28      0.28       250\n",
      "          5       0.27      0.14      0.19       221\n",
      "          6       0.22      0.31      0.26       213\n",
      "          7       0.27      0.29      0.28       265\n",
      "          8       0.30      0.21      0.25       264\n",
      "          9       0.24      0.22      0.23       251\n",
      "\n",
      "avg / total       0.31      0.31      0.30      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(best_param_set[0],best_param_set[1],best_param_set[2],best_param_set[3],best_param_set[4],best_param_set[5])\n",
    "nn.set_weight(0.2)\n",
    "nn.fitNN(X_train,labels_train)\n",
    "\n",
    "predictions = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    o = nn.predict(X_test[i] )\n",
    "    predictions.append(np.argmax(o))\n",
    "print (confusion_matrix(y_test,predictions))\n",
    "print (classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\lib\\site-packages\\matplotlib\\collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAD0CAYAAAB97VinAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x585dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,predictions)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
